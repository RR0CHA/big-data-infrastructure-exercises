============================= test session starts ==============================
collecting ... collected 19 items

evaluation/s1/test_s1.py::TestEvaluateS1::test_download PASSED           [  5%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_1000_files PASSED         [ 10%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_preparation PASSED        [ 15%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_aircrafts_schema PASSED   [ 21%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_all_aircrafts_no_duplicates PASSED [ 26%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_all_aircrafts_count FAILED [ 31%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_paging[138] PASSED [ 36%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_paging[20] PASSED [ 42%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[040014-expected0] FAILED [ 47%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[a095f0-expected1] FAILED [ 52%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[abf2c2-expected2] FAILED [ 57%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_stats_schema PASSED [ 63%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_ordered_asc[a972d3] FAILED [ 68%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_non_existent_position PASSED [ 73%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_position_schema PASSED [ 78%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_positions[040014-199] PASSED [ 84%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_positions[a972d3-968] PASSED [ 89%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_num_results[10] PASSED [ 94%]
evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_num_results[720] PASSED [100%]

=================================== FAILURES ===================================
___________________ TestEvaluateS1.test_all_aircrafts_count ____________________

self = <s1.test_s1.TestEvaluateS1 object at 0x7f47c0332510>
client = <starlette.testclient.TestClient object at 0x7f47c0255cd0>
json_metadata = {'name': 'test_all_aircrafts_count', 'time': 932006}
request = <FixtureRequest for <Function test_all_aircrafts_count>>

    def test_all_aircrafts_count(self, client, json_metadata, request) -> None:
        start = int(round(time.time() * 1000))
        # Given
        # When
        results = []
    
        with client as client:  # Always do this
            page = 0
            r = client.get(f"/api/s1/aircraft/?num_results=500&page={page}")
            assert r.status_code == 200
            response = r.json()
            results.extend(a["icao"] for a in response)
    
            while response and len(response) == 500 and page < 100:
                page += 1
                r = client.get(f"/api/s1/aircraft/?num_results=500&page={page}")
                assert r.status_code == 200
                response = r.json()
                results.extend(a["icao"] for a in response)
    
        elapsed = int(round(time.time() * 1000)) - start
        add_metadata(json_metadata, request, elapsed)
        # Then
>       assert len(results) == 36807
E       AssertionError: assert 18000 == 36807
E        +  where 18000 = len(['008080', '0099ea', '00b1f4', '00b1fb', '00b2df', '00b2f0', ...])

evaluation/s1/test_s1.py:121: AssertionError
_______________ TestEvaluateS1.test_statistics[040014-expected0] _______________

self = <s1.test_s1.TestEvaluateS1 object at 0x7f47c031e250>
client = <starlette.testclient.TestClient object at 0x7f47c0255cd0>
icao = '040014'
expected = {'had_emergency': False, 'max_altitude_baro': 25850, 'max_ground_speed': 460}
json_metadata = {'name': 'test_statistics', 'time': 22667}
request = <FixtureRequest for <Function test_statistics[040014-expected0]>>

    @pytest.mark.parametrize(
        "icao,expected",
        [
            (
                "040014",
                {
                    "max_altitude_baro": 25850,
                    "max_ground_speed": 460,
                    "had_emergency": False,
                },
            ),
            (
                "a095f0",
                {
                    "max_altitude_baro": 28000,
                    "max_ground_speed": 392.6,
                    "had_emergency": False,
                },
            ),
            (
                "abf2c2",
                {
                    "max_altitude_baro": 29200,
                    "max_ground_speed": 473.8,
                    "had_emergency": True,
                },
            ),
        ],
    )
    def test_statistics(self, client, icao, expected, json_metadata, request) -> None:
        start = int(round(time.time() * 1000))
        # Given
        # When
        with client as client:  # Always do this
            response = client.get(f"/api/s1/aircraft/{icao}/stats")
        elapsed = int(round(time.time() * 1000)) - start
        add_metadata(json_metadata, request, elapsed)
        # Then
>       assert response.json() == expected
E       AssertionError: assert {'had_emergen...speed': 460.0} == {'had_emergen...d_speed': 460}
E         Omitting 2 identical items, use -vv to show
E         Differing items:
E         {'had_emergency': 'False'} != {'had_emergency': False}
E         Full diff:
E           {
E         -  'had_emergency': False,
E         +  'had_emergency': 'False',...
E         
E         ...Full output truncated (8 lines hidden), use '-vv' to show

evaluation/s1/test_s1.py:173: AssertionError
_______________ TestEvaluateS1.test_statistics[a095f0-expected1] _______________

self = <s1.test_s1.TestEvaluateS1 object at 0x7f47c031e450>
client = <starlette.testclient.TestClient object at 0x7f47c0255cd0>
icao = 'a095f0'
expected = {'had_emergency': False, 'max_altitude_baro': 28000, 'max_ground_speed': 392.6}
json_metadata = {'name': 'test_statistics', 'time': 22785}
request = <FixtureRequest for <Function test_statistics[a095f0-expected1]>>

    @pytest.mark.parametrize(
        "icao,expected",
        [
            (
                "040014",
                {
                    "max_altitude_baro": 25850,
                    "max_ground_speed": 460,
                    "had_emergency": False,
                },
            ),
            (
                "a095f0",
                {
                    "max_altitude_baro": 28000,
                    "max_ground_speed": 392.6,
                    "had_emergency": False,
                },
            ),
            (
                "abf2c2",
                {
                    "max_altitude_baro": 29200,
                    "max_ground_speed": 473.8,
                    "had_emergency": True,
                },
            ),
        ],
    )
    def test_statistics(self, client, icao, expected, json_metadata, request) -> None:
        start = int(round(time.time() * 1000))
        # Given
        # When
        with client as client:  # Always do this
            response = client.get(f"/api/s1/aircraft/{icao}/stats")
        elapsed = int(round(time.time() * 1000)) - start
        add_metadata(json_metadata, request, elapsed)
        # Then
>       assert response.json() == expected
E       AssertionError: assert {'had_emergen...speed': 392.6} == {'had_emergen...speed': 392.6}
E         Omitting 2 identical items, use -vv to show
E         Differing items:
E         {'had_emergency': 'False'} != {'had_emergency': False}
E         Full diff:
E           {
E         -  'had_emergency': False,
E         +  'had_emergency': 'False',...
E         
E         ...Full output truncated (6 lines hidden), use '-vv' to show

evaluation/s1/test_s1.py:173: AssertionError
_______________ TestEvaluateS1.test_statistics[abf2c2-expected2] _______________

self = <s1.test_s1.TestEvaluateS1 object at 0x7f47c031e710>
client = <starlette.testclient.TestClient object at 0x7f47c0255cd0>
icao = 'abf2c2'
expected = {'had_emergency': True, 'max_altitude_baro': 29200, 'max_ground_speed': 473.8}
json_metadata = {'name': 'test_statistics', 'time': 22882}
request = <FixtureRequest for <Function test_statistics[abf2c2-expected2]>>

    @pytest.mark.parametrize(
        "icao,expected",
        [
            (
                "040014",
                {
                    "max_altitude_baro": 25850,
                    "max_ground_speed": 460,
                    "had_emergency": False,
                },
            ),
            (
                "a095f0",
                {
                    "max_altitude_baro": 28000,
                    "max_ground_speed": 392.6,
                    "had_emergency": False,
                },
            ),
            (
                "abf2c2",
                {
                    "max_altitude_baro": 29200,
                    "max_ground_speed": 473.8,
                    "had_emergency": True,
                },
            ),
        ],
    )
    def test_statistics(self, client, icao, expected, json_metadata, request) -> None:
        start = int(round(time.time() * 1000))
        # Given
        # When
        with client as client:  # Always do this
            response = client.get(f"/api/s1/aircraft/{icao}/stats")
        elapsed = int(round(time.time() * 1000)) - start
        add_metadata(json_metadata, request, elapsed)
        # Then
>       assert response.json() == expected
E       AssertionError: assert {'had_emergen...speed': 473.8} == {'had_emergen...speed': 473.8}
E         Omitting 2 identical items, use -vv to show
E         Differing items:
E         {'had_emergency': 'True'} != {'had_emergency': True}
E         Full diff:
E           {
E         -  'had_emergency': True,
E         +  'had_emergency': 'True',...
E         
E         ...Full output truncated (6 lines hidden), use '-vv' to show

evaluation/s1/test_s1.py:173: AssertionError
______________ TestEvaluateS1.test_positions_ordered_asc[a972d3] _______________

self = <s1.test_s1.TestEvaluateS1 object at 0x7f47c031eed0>
client = <starlette.testclient.TestClient object at 0x7f47c0255cd0>
icao = 'a972d3'
json_metadata = {'name': 'test_positions_ordered_asc', 'time': 23538}
request = <FixtureRequest for <Function test_positions_ordered_asc[a972d3]>>

    @pytest.mark.parametrize("icao", ["a972d3"])
    def test_positions_ordered_asc(self, client, icao, json_metadata, request) -> None:
        start = int(round(time.time() * 1000))
        # Given
        # When
        results = []
        with client as client:  # Always do this
            page = 0
            r = client.get(
                f"/api/s1/aircraft/{icao}/positions?num_results=1000&page={page}"
            )
            assert r.status_code == 200
            response = r.json()
            results.extend(a["timestamp"] for a in response)
            while response and len(response) == 1000 and page < 100:
                page += 1
                r = client.get(
                    f"/api/s1/aircraft/{icao}/positions?num_results=1000&page={page}"
                )
                assert r.status_code == 200
                response = r.json()
                results.extend(a["timestamp"] for a in response)
        elapsed = int(round(time.time() * 1000)) - start
        add_metadata(json_metadata, request, elapsed)
        # Then
        assert len(results) > 1
>       assert sorted(results.copy()) == results
E       TypeError: '<' not supported between instances of 'NoneType' and 'float'

evaluation/s1/test_s1.py:223: TypeError
==================================== PASSES ====================================
_________________________ TestEvaluateS1.test_download _________________________
----------------------------- Captured stdout call -----------------------------
->> Init download
->> Clear download
->> Ended
--------------------------------- JSON report ----------------------------------
report saved to: .pytest.json
=========================== short test summary info ============================
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_download
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_1000_files
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_preparation
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_aircrafts_schema
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_all_aircrafts_no_duplicates
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_paging[138]
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_paging[20]
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_stats_schema
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_non_existent_position
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_aircraft_position_schema
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_positions[040014-199]
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_positions[a972d3-968]
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_num_results[10]
PASSED evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_num_results[720]
FAILED evaluation/s1/test_s1.py::TestEvaluateS1::test_all_aircrafts_count - A...
FAILED evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[040014-expected0]
FAILED evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[a095f0-expected1]
FAILED evaluation/s1/test_s1.py::TestEvaluateS1::test_statistics[abf2c2-expected2]
FAILED evaluation/s1/test_s1.py::TestEvaluateS1::test_positions_ordered_asc[a972d3]
================== 5 failed, 14 passed in 1962.78s (0:32:42) ===================
============================= test session starts ==============================
collecting ... collected 11 items / 5 deselected / 6 selected

tests/s1/test_s1.py::TestS1Student::test_download1 PASSED
tests/s1/test_s1.py::TestS1Student::test_download2 ->> Init download
->> Clear download
->> Ended
PASSED
tests/s1/test_s1.py::TestS1Student::test_prepare1 PASSED
tests/s1/test_s1.py::TestS1Student::test_aircraft1 PASSED
tests/s1/test_s1.py::TestS1Student::test_positions1 PASSED
tests/s1/test_s1.py::TestS1Student::test_stats1 PASSED

==================================== PASSES ====================================
--------------------------------- JSON report ----------------------------------
report saved to: .pytest.student.json

---------- coverage: platform linux, python 3.11.2-final-0 -----------
Coverage JSON written to file .coverage.json

=========================== short test summary info ============================
PASSED tests/s1/test_s1.py::TestS1Student::test_download1
PASSED tests/s1/test_s1.py::TestS1Student::test_download2
PASSED tests/s1/test_s1.py::TestS1Student::test_prepare1
PASSED tests/s1/test_s1.py::TestS1Student::test_aircraft1
PASSED tests/s1/test_s1.py::TestS1Student::test_positions1
PASSED tests/s1/test_s1.py::TestS1Student::test_stats1
======================= 6 passed, 5 deselected in 9.36s ========================
